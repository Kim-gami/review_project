# -*- coding: utf-8 -*-
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, ElementClickInterceptedException,
    StaleElementReferenceException
)
import os, re, time
from datetime import datetime

# ğŸ”— ì •ìë™ ì‹ë‹¹ ê²€ìƒ‰ URL
SEARCH_URL = "https://map.naver.com/p/search/%EC%A0%95%EC%9E%90%EB%8F%99%20%EC%8B%9D%EB%8B%B9?c=14.00,0,0,0,dh"
SAVE_DIR = "saved_pages"

WAIT_LONG = 10
WAIT_SHORT = 8

def make_driver():
    opts = webdriver.ChromeOptions()
    opts.add_experimental_option("detach", True)   # ìŠ¤í¬ë¦½íŠ¸ ëë‚˜ë„ ì°½ ìœ ì§€
    driver = webdriver.Chrome(options=opts)
    driver.set_window_size(1300, 950)
    return driver

def wwait(drv, t=WAIT_LONG, poll=0.2):
    return WebDriverWait(drv, t, poll_frequency=poll)

def click_with_retry(drv, by, sel, tries=4, pre_sleep=0.2, post_sleep=0.35, timeout=WAIT_SHORT):
    last_err = None
    for attempt in range(1, tries+1):
        try:
            if pre_sleep: time.sleep(pre_sleep)
            w = wwait(drv, timeout)
            el = w.until(EC.presence_of_element_located((by, sel)))
            el = w.until(EC.visibility_of_element_located((by, sel)))
            drv.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
            w.until(EC.element_to_be_clickable((by, sel)))
            try:
                ActionChains(drv).move_to_element(el).pause(0.05).click(el).perform()
            except (ElementClickInterceptedException, StaleElementReferenceException):
                drv.execute_script("arguments[0].click();", el)
            if post_sleep: time.sleep(post_sleep)
            return el
        except Exception as e:
            last_err = e
            time.sleep(0.5 + 0.25*attempt)
    raise TimeoutException(f"click_with_retry ì‹¤íŒ¨: {sel} // {last_err}")

def to_search_iframe(drv):
    drv.switch_to.default_content()
    wwait(drv).until(EC.presence_of_element_located((By.ID, "searchIframe")))
    drv.switch_to.frame(drv.find_element(By.ID, "searchIframe"))

def to_entry_iframe(drv):
    drv.switch_to.default_content()
    wwait(drv).until(EC.presence_of_element_located((By.ID, "entryIframe")))
    drv.switch_to.frame(drv.find_element(By.ID, "entryIframe"))
    time.sleep(0.5)

def wait_entry_loaded(drv, timeout=WAIT_LONG):
    w = wwait(drv, timeout)
    try:
        w.until(EC.presence_of_element_located((By.XPATH, "//section[contains(@class,'place_section')]")))
    except TimeoutException:
        time.sleep(0.8)

def wait_reviews_ready(drv, timeout=WAIT_SHORT):
    w = wwait(drv, timeout)
    try:
        w.until(lambda d: d.execute_script("return document.readyState") == "complete")
    except Exception:
        pass
    time.sleep(0.6)

def save_html(drv, label="page"):
    os.makedirs(SAVE_DIR, exist_ok=True)
    ts = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    safe_label = re.sub(r"[^0-9A-Za-zê°€-í£._-]", "_", label)[:80] or "page"
    path = os.path.join(SAVE_DIR, f"{ts}_{safe_label}.html")
    with open(path, "w", encoding="utf-8") as f:
        f.write(drv.page_source)
    print(f"[SAVE] {path}")
    return path

def wait_review_container(drv, timeout=20):
    candidates = [
        (By.CSS_SELECTOR, "#_review_list"),
        (By.CSS_SELECTOR, "div[id$='_review_list']"),
        (By.CSS_SELECTOR, "section[class*='place_section'][data-tab='review']"),
        (By.CSS_SELECTOR, "div.place_section_content"),
    ]
    w = wwait(drv, timeout, poll=0.2)
    for by, sel in candidates:
        try:
            el = w.until(EC.presence_of_element_located((by, sel)))
            drv.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
            return el
        except Exception:
            continue
    raise TimeoutException("ë¦¬ë·° ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")

def save_review_section_html(drv, label="review_section"):
    os.makedirs(SAVE_DIR, exist_ok=True)
    to_entry_iframe(drv)
    el = wait_review_container(drv, timeout=20)
    html = el.get_attribute("outerHTML")
    ts = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    safe_label = re.sub(r"[^0-9A-Za-zê°€-í£._-]", "_", label)[:80] or "review_section"
    path = os.path.join(SAVE_DIR, f"{ts}_{safe_label}.html")
    with open(path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"[SAVE] ë¦¬ë·° ì„¹ì…˜ â†’ {path}")
    return path

# --- ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ---
def list_search_items(drv, timeout=WAIT_LONG):
    """ê²€ìƒ‰ iframeì—ì„œ ê°€ê²Œëª… span ë¦¬ìŠ¤íŠ¸ë¥¼ (ì´ë¦„, locator) ë°°ì—´ë¡œ ë°˜í™˜."""
    to_search_iframe(drv)
    w = wwait(drv, timeout)
    w.until(EC.presence_of_all_elements_located((By.XPATH, "//span[contains(@class,'TYaxT')]")))
    spans = drv.find_elements(By.XPATH, "//span[contains(@class,'TYaxT')]")
    items = []
    for idx, el in enumerate(spans):
        try:
            name = el.text.strip()
            if not name:
                continue
            xp = f"(//span[contains(@class,'TYaxT')])[{idx+1}]"
            items.append((name, (By.XPATH, xp)))
        except Exception:
            continue
    if not items:
        raise TimeoutException("ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return items

def open_reviews_tab_and_sort(drv, tries=4, sort_latest=True):
    """ìƒì„¸ì—ì„œ ë¦¬ë·° íƒ­ â†’ (ì˜µì…˜) ìµœì‹ ìˆœ ì •ë ¬."""
    to_entry_iframe(drv)
    wait_entry_loaded(drv)

    print("[CLICK] ë¦¬ë·° íƒ­")
    click_with_retry(
        drv, By.XPATH,
        "//span[contains(@class,'veBoZ') and normalize-space()='ë¦¬ë·°']",
        tries=max(tries, 3), pre_sleep=0.4, post_sleep=0.8, timeout=WAIT_LONG
    )
    wait_reviews_ready(drv)

    if sort_latest:
        print("[CLICK] ë¦¬ë·° ì •ë ¬(ìµœì‹ ìˆœ)")
        try:
            click_with_retry(
                drv, By.XPATH,
                "//a[@role='option' and contains(@class,'place_btn_option') and normalize-space()='ìµœì‹ ìˆœ']",
                tries=3, pre_sleep=0.3, post_sleep=0.6, timeout=WAIT_SHORT
            )
        except Exception:
            try:
                click_with_retry(
                    drv, By.XPATH,
                    "//button[contains(@class,'place_section_content')]//span[contains(text(),'ì •ë ¬')]",
                    tries=2, pre_sleep=0.2, post_sleep=0.4, timeout=WAIT_SHORT
                )
                click_with_retry(
                    drv, By.XPATH,
                    "//a[@role='option' and contains(@class,'place_btn_option') and normalize-space()='ìµœì‹ ìˆœ']",
                    tries=3, pre_sleep=0.2, post_sleep=0.6, timeout=WAIT_SHORT
                )
            except Exception:
                print("[WARN] ìµœì‹ ìˆœ ì„ íƒ ì‹¤íŒ¨ â€” ê³„ì† ì§„í–‰")
    return True

def expand_all_reviews(drv, max_clicks=300, scroll_step=500, pause=0.25):
    """
    ìƒì„¸ iframeì—ì„œ ë¦¬ë·° ë³¸ë¬¸ 'ë”ë³´ê¸°(rvshowmore)'ë§Œ ëª¨ë‘ í¼ì¹¨.
    í´ë¦­í•œ ë²„íŠ¼ì€ ì¦‰ì‹œ DOMì—ì„œ ì œê±°í•´ ì¬í´ë¦­(ì ‘í˜) ë°©ì§€.
    """
    to_entry_iframe(drv)
    total_clicked = 0
    idle_rounds = 0

    while total_clicked < max_clicks and idle_rounds < 10:
        clicked_this_round = 0

        containers = drv.find_elements(By.CSS_SELECTOR, "#_review_list, div[id$='_review_list']")
        scope = containers[0] if containers else drv

        btns = scope.find_elements(By.CSS_SELECTOR, "a[data-pui-click-code='rvshowmore']")
        for b in btns:
            try:
                if not b.is_displayed():
                    continue
                txt = (b.text or "").strip()
                if "ì ‘ê¸°" in txt:
                    continue

                drv.execute_script("arguments[0].scrollIntoView({block:'center'});", b)
                time.sleep(0.05)
                try:
                    ActionChains(drv).move_to_element(b).pause(0.02).click(b).perform()
                except (ElementClickInterceptedException, StaleElementReferenceException):
                    drv.execute_script("arguments[0].click();", b)

                try:
                    drv.execute_script("""
                        if (arguments[0]) {
                            arguments[0].setAttribute('data-expanded-once','1');
                            arguments[0].parentNode && arguments[0].parentNode.removeChild(arguments[0]);
                        }
                    """, b)
                except Exception:
                    pass

                total_clicked += 1
                clicked_this_round += 1
                time.sleep(0.12)
                if total_clicked >= max_clicks:
                    break
            except Exception:
                continue

        if clicked_this_round == 0:
            idle_rounds += 1
            drv.execute_script(f"window.scrollBy(0, {scroll_step});")
            time.sleep(pause)
        else:
            idle_rounds = 0

    print(f"[INFO] ë¦¬ë·° ë”ë³´ê¸° í¼ì¹¨ ì™„ë£Œ: {total_clicked}íšŒ, idle_rounds={idle_rounds}")

# --- í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ---
def goto_search_page(drv, page_no, tries=3):
    """
    ê²€ìƒ‰ iframeì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ í´ë¦­ (ì˜ˆ: <a role="button" class="mBN2s ">2</a>)
    """
    to_search_iframe(drv)
    xp = f"//a[@role='button' and contains(@class,'mBN2s') and normalize-space()='{page_no}']"
    print(f"[PAGE] {page_no}í˜ì´ì§€ ì´ë™ ì‹œë„")
    for attempt in range(1, tries+1):
        try:
            click_with_retry(drv, By.XPATH, xp, tries=2, pre_sleep=0.2, post_sleep=0.6, timeout=WAIT_LONG)
            # ìƒˆ í˜ì´ì§€ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
            wwait(drv, WAIT_LONG).until(
                EC.presence_of_all_elements_located((By.XPATH, "//span[contains(@class,'TYaxT')]"))
            )
            time.sleep(0.5)
            print(f"[PAGE] {page_no}í˜ì´ì§€ ì´ë™ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"[WARN] {page_no}í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {attempt}) // {e}")
            time.sleep(1.0)
    return False

from selenium.webdriver.common.keys import Keys

def count_search_items(drv):
    """ê²€ìƒ‰ iframeì—ì„œ ë¦¬ìŠ¤íŠ¸ í•­ëª© ê°œìˆ˜ ë°˜í™˜."""
    to_search_iframe(drv)
    els = drv.find_elements(By.XPATH, "//span[contains(@class,'TYaxT')]")
    return len([e for e in els if (e.text or "").strip()])

def _find_search_list_container(drv):
    """
    ê²€ìƒ‰ iframe ì•ˆì—ì„œ ì‹¤ì œë¡œ ìŠ¤í¬ë¡¤ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆë¥¼ ìµœëŒ€í•œ ë³´í¸ì ìœ¼ë¡œ íƒìƒ‰.
    - overflowYê°€ 'auto'/'scroll'ì´ê³ , ì•„ì´í…œ(span.TYaxT)ì„ í¬í•¨í•˜ëŠ” ê°€ì¥ í° divë¥¼ ì„ íƒ
    """
    to_search_iframe(drv)
    divs = drv.find_elements(By.CSS_SELECTOR, "div")
    candidate = None
    max_area = 0
    for d in divs:
        try:
            has_item = len(d.find_elements(By.XPATH, ".//span[contains(@class,'TYaxT')]")) > 0
            if not has_item:
                continue
            overflow = drv.execute_script("return window.getComputedStyle(arguments[0]).overflowY;", d)
            if overflow not in ("auto", "scroll"):
                continue
            h = d.size.get("height", 0)
            w = d.size.get("width", 0)
            area = h * w
            if area > max_area:
                max_area = area
                candidate = d
        except Exception:
            continue
    return candidate  # ì—†ìœ¼ë©´ None ë°˜í™˜

def scroll_search_list_until_stable(
    drv,
    max_tries=80,         # â¬… ë” ê¹Šê²Œ: ì‹œë„ íšŸìˆ˜ ì¦ê°€
    pause=0.45,           # â¬… ë¡œë”© ì—¬ìœ  ì¦ê°€
    stable_needed=4,      # â¬… ë³€í™” ì—†ìŒì„ ë” ì—„ê²©íˆ
    extra_push=True       # â¬… í‚¤ë³´ë“œ/íœ  ì´ë²¤íŠ¸ë¡œ ì¶”ê°€ íŠ¸ë¦¬ê±°
):
    """
    ê²€ìƒ‰ iframeì—ì„œ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆì— ìŠ¤í¬ë¡¤ì„ ê°€í•´
    ë§¤ì¥ ìˆ˜ê°€ ë” ì´ìƒ ëŠ˜ì–´ë‚˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë‚´ë ¤ê°.
    """
    to_search_iframe(drv)
    container = _find_search_list_container(drv)
    prev_cnt = -1
    stable = 0

    for i in range(max_tries):
        # í˜„ì¬ ê°œìˆ˜
        cur_cnt = count_search_items(drv)

        if cur_cnt == prev_cnt:
            stable += 1
        else:
            stable = 0

        if stable >= stable_needed:
            print(f"[SCROLL] ë¦¬ìŠ¤íŠ¸ ì•ˆì •í™”: {cur_cnt}ê°œ (ì‹œë„ {i+1})")
            return cur_cnt

        # ë§ˆì§€ë§‰ ì•„ì´í…œì„ ê¸°ì¤€ìœ¼ë¡œ scrollIntoView (ë¡œë”© íŠ¸ë¦¬ê±° ê°€ì¥ í™•ì‹¤)
        try:
            items = drv.find_elements(By.XPATH, "//span[contains(@class,'TYaxT')]")
            if items:
                last_el = items[-1]
                drv.execute_script("arguments[0].scrollIntoView({block:'end'});", last_el)
        except Exception:
            pass

        # ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆì— ì§ì ‘ ìŠ¤í¬ë¡¤ (ì°½ì´ ì•„ë‹ˆë¼ ì»¨í…Œì´ë„ˆ!)
        try:
            if container:
                drv.execute_script("arguments[0].scrollTop = arguments[0].scrollTop + arguments[0].clientHeight;", container)
        except Exception:
            pass

        # ì¶”ê°€ í‘¸ì‹œ(ì„ íƒ): í‚¤ë³´ë“œ/íœ  ì´ë²¤íŠ¸ë¡œ lazy-load ë” ìê·¹
        if extra_push:
            try:
                if container:
                    # íœ  ì´ë²¤íŠ¸
                    drv.execute_script("""
                        const el = arguments[0];
                        el.dispatchEvent(new WheelEvent('wheel', {deltaY: el.clientHeight}));
                    """, container)
                else:
                    # ì»¨í…Œì´ë„ˆë¥¼ ëª» ì°¾ì€ ê²½ìš°ì—ëŠ” ë¬¸ì„œì— PageDown
                    body = drv.find_element(By.TAG_NAME, "body")
                    body.send_keys(Keys.PAGE_DOWN)
            except Exception:
                pass

        time.sleep(pause)
        prev_cnt = cur_cnt

    final_cnt = count_search_items(drv)
    print(f"[SCROLL] ìµœëŒ€ ì‹œë„ ë„ë‹¬: {final_cnt}ê°œ")
    return final_cnt



# --- ë©”ì¸ ë£¨í”„ ---
def run_crawl(max_pages=5, per_page_limit=None, sort_latest=True, expand_more=True):
    drv = make_driver()
    drv.get(SEARCH_URL)

    total_processed = 0

    for page in range(1, max_pages + 1):
        ok = goto_search_page(drv, page)
        if not ok:
            print(f"[WARN] {page}í˜ì´ì§€ë¡œ ì´ë™í•˜ì§€ ëª»í•´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # âœ… í˜ì´ì§€ ë„˜ê¸°ê¸° ì „ì— ëê¹Œì§€ ìŠ¤í¬ë¡¤í•´ì„œ ë§¤ì¥ ìµœëŒ€ ë…¸ì¶œ
        total_on_page = scroll_search_list_until_stable(drv)
        print(f"[INFO] {page}í˜ì´ì§€: ìŠ¤í¬ë¡¤ í›„ ë§¤ì¥ {total_on_page}ê°œ ê°ì§€")

        # ì´ì œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•´ì„œ ì²˜ë¦¬
        items = list_search_items(drv)
        print(f"[INFO] {page}í˜ì´ì§€: ì‹¤ì œ ìˆ˜ì§‘ ëŒ€ìƒ {len(items)}ê°œ")

        processed_this_page = 0
        for idx, (name, locator) in enumerate(items, start=1):
            if per_page_limit and processed_this_page >= per_page_limit:
                break

            to_search_iframe(drv)
            print(f"\n[CLICK] [{page}í˜ì´ì§€ #{idx}] ê°€ê²Œ ì„ íƒ: {name}")
            try:
                click_with_retry(drv, locator[0], locator[1], tries=4, pre_sleep=0.2, post_sleep=0.6, timeout=WAIT_LONG)
            except Exception as e:
                print(f"[WARN] ê°€ê²Œ ì„ íƒ ì‹¤íŒ¨: {name} // {e}")
                continue

            try:
                open_reviews_tab_and_sort(drv, tries=4, sort_latest=sort_latest)
                if expand_more:
                    expand_all_reviews(drv, max_clicks=200)
            except Exception as e:
                print(f"[WARN] ë¦¬ë·° íƒ­/ì •ë ¬/ë”ë³´ê¸° ì²˜ë¦¬ ì¤‘ ë¬¸ì œ: {name} // {e}")

            safe_name = re.sub(r"[^0-9A-Za-zê°€-í£._-]", "_", name)[:60] or f"place_{idx}"
            drv.switch_to.default_content()
            # save_html(drv, f"p{page:02d}_{idx:02d}_{safe_name}_fullpage")
            try:
                save_review_section_html(drv, f"p{page:02d}_{idx:02d}_{safe_name}_reviews")
            except Exception as e:
                print(f"[WARN] ë¦¬ë·° ì„¹ì…˜ ì €ì¥ ì‹¤íŒ¨: {name} // {e}")

            processed_this_page += 1
            total_processed += 1

        print(f"\n[PAGE DONE] {page}í˜ì´ì§€ ì²˜ë¦¬: {processed_this_page}ê°œ")

    print(f"\n[DONE] ì´ ì²˜ë¦¬ ê°€ê²Œ ìˆ˜: {total_processed}ê°œ")
    input("ë¸Œë¼ìš°ì €ëŠ” ì—´ë¦° ìƒíƒœë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ Enter...")

if __name__ == "__main__":
    # 1~5í˜ì´ì§€, í˜ì´ì§€ë‹¹ ì „ë¶€ ì²˜ë¦¬, ìµœì‹ ìˆœ ì •ë ¬ + ë”ë³´ê¸° í¼ì¹˜ê¸°
    run_crawl(max_pages=5, per_page_limit=None, sort_latest=True, expand_more=True)
