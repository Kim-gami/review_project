from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, ElementClickInterceptedException, StaleElementReferenceException
)
import os, re
from datetime import datetime

# ğŸ”— ì •ìë™ ì‹ë‹¹ ê²€ìƒ‰ URL
SEARCH_URL = "https://www.google.co.kr/maps/search/%EC%A0%95%EC%9E%90%EB%8F%99+%EC%8B%9D%EB%8B%B9/data=!3m1!4b1?entry=ttu&g_ep=EgoyMDI1MDgxOS4wIKXMDSoASAFQAw%3D%3D"

# â›³ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ê° ê°€ê²Œ ì¹´ë“œ anchor
CSS_RESULT_LINKS = "a.hfpxzc[aria-label][href]"

# â­ ìƒì„¸ì—ì„œ ë¦¬ë·° íƒ­ ë²„íŠ¼(â€˜ë¦¬ë·°â€™ divë¥¼ ê°ì‹¸ëŠ” button)
XPATH_REVIEW_BUTTON = "//button[.//div[normalize-space()='ë¦¬ë·°']]"

SAVE_DIR = "saved_pages"

def make_driver():
    opts = webdriver.ChromeOptions()
    opts.add_experimental_option("detach", True)   # ìŠ¤í¬ë¦½íŠ¸ ëë‚˜ë„ ì°½ ìœ ì§€
    driver = webdriver.Chrome(options=opts)
    driver.set_window_size(1300, 950)
    return driver

def wwait(driver, timeout=20, poll=0.2):
    return WebDriverWait(driver, timeout, poll_frequency=poll)

def safe_click(driver, locator, timeout=20):
    wait = wwait(driver, timeout)
    el = wait.until(EC.presence_of_element_located(locator))
    el = wait.until(EC.visibility_of_element_located(locator))
    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
    wait.until(EC.element_to_be_clickable(locator))
    try:
        ActionChains(driver).move_to_element(el).click(el).perform()
    except (ElementClickInterceptedException, StaleElementReferenceException):
        el = driver.find_element(*locator)
        driver.execute_script("arguments[0].click();", el)
    return True

def get_results_container(driver):
    """
    êµ¬ê¸€ë§µ ì¢Œì¸¡ ê²°ê³¼ íŒ¨ë„ì˜ ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ëŠ”ë‹¤.
    ìš°ì„ ìˆœìœ„:
      1) role='feed' (ìš”ì¦˜ ì§€ë„ DOMì—ì„œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
      2) ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ m6QErb... ì»¨í…Œì´ë„ˆ (ë°±ì—…)
    """
    wait = wwait(driver, 20)
    try:
        feed = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div[role='feed']")))
        return feed
    except TimeoutException:
        pass
    # ë°±ì—… ì…€ë ‰í„° (í´ë˜ìŠ¤ëŠ” ìì£¼ ë³€í•´ì„œ contains í™œìš©)
    backup = wait.until(EC.presence_of_element_located((
        By.CSS_SELECTOR,
        "div.m6QErb.DxyBCb.kA9KIf.dS8AEf"  # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ íŒ¨ë„ì˜ ê³µí†µ ì¡°í•©
    )))
    return backup

def scroll_results_to_load_more(driver, max_scrolls=40, min_increment=3, timeout_each=10):
    """
    ê²°ê³¼ íŒ¨ë„ì„ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ë©° ë” ë§ì€ ê°€ê²Œ ì¹´ë“œë¥¼ ë¡œë”©í•œë‹¤.
    - max_scrolls: ìµœëŒ€ ìŠ¤í¬ë¡¤ ì‹œë„ íšŸìˆ˜
    - min_increment: ìƒˆë¡œ ë¡œë“œë˜ì—ˆë‹¤ê³  ì¸ì •í•  ìµœì†Œ ê²°ê³¼ ì¦ê°€ ìˆ˜
    - timeout_each: ê° ìŠ¤í¬ë¡¤ ì´í›„ ì¦ê°€ ê°ì§€ ëŒ€ê¸° ìµœëŒ€ ì´ˆ(Wait ê¸°ë°˜)
    """
    container = get_results_container(driver)
    wait = wwait(driver, timeout_each, poll=0.2)

    def current_count():
        return len(driver.find_elements(By.CSS_SELECTOR, CSS_RESULT_LINKS))

    seen_count = current_count()
    stagnant = 0

    for i in range(1, max_scrolls + 1):
        # ì»¨í…Œì´ë„ˆ ìµœí•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", container)

        try:
            # ê²°ê³¼ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, CSS_RESULT_LINKS)) >= seen_count + min_increment)
            new_count = current_count()
            print(f"[SCROLL {i}] ê²°ê³¼ {seen_count} â†’ {new_count}")
            if new_count == seen_count:
                stagnant += 1
            else:
                stagnant = 0
            seen_count = new_count
        except TimeoutException:
            # ì¦ê°€ ì—†ìœ¼ë©´ ì •ì²´ ì¹´ìš´íŠ¸ ì¦ê°€
            stagnant += 1
            print(f"[SCROLL {i}] ì¦ê°€ ì—†ìŒ (ëˆ„ì  ì •ì²´ {stagnant})")

        # ëª‡ ë²ˆ ì—°ì†ìœ¼ë¡œ ì¦ê°€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if stagnant >= 3:
            print("[INFO] ì¶”ê°€ ë¡œë”© ì •ì²´: ìŠ¤í¬ë¡¤ ì¤‘ë‹¨")
            break

def collect_result_links(driver, max_items=None):
    """
    í™”ë©´ì— ë¡œë“œëœ a.hfpxzc ë§í¬ë“¤ì„ ìˆ˜ì§‘í•´ (href, label) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """
    wwait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, CSS_RESULT_LINKS)))
    anchors = driver.find_elements(By.CSS_SELECTOR, CSS_RESULT_LINKS)
    seen = set()
    items = []
    for a in anchors:
        try:
            href = a.get_attribute("href")
            label = (a.get_attribute("aria-label") or "").split("Â·", 1)[0].strip()
            if not href or href in seen:
                continue
            seen.add(href)
            items.append((href, label))
            if max_items and len(items) >= max_items:
                break
        except Exception:
            continue
    return items

def save_current_html(driver, label="page"):
    os.makedirs(SAVE_DIR, exist_ok=True)
    ts = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    safe_label = re.sub(r"[^0-9A-Za-zê°€-í£._-]", "_", label)[:80] or "page"
    html_path = os.path.join(SAVE_DIR, f"{ts}_{safe_label}.html")
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(driver.page_source)
    print(f"[SAVE] HTML â†’ {html_path}")
    return html_path

def click_reviews(driver, timeout=20):
    return safe_click(driver, (By.XPATH, XPATH_REVIEW_BUTTON), timeout=timeout)

def run(max_items=None, do_scroll=True, scroll_batches=40):
    """
    1) ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì†
    2) (ì˜µì…˜) ê²°ê³¼ íŒ¨ë„ ìŠ¤í¬ë¡¤ë¡œ ì¶”ê°€ ë¡œë”©
    3) ê²°ê³¼ ë§í¬ ìˆ˜ì§‘
    4) ê° ë§í¬ë¡œ ì´ë™ â†’ 'ë¦¬ë·°' í´ë¦­ â†’ HTML ì €ì¥
    """
    driver = make_driver()
    driver.get(SEARCH_URL)

    # 2) ìŠ¤í¬ë¡¤ë¡œ ë” ë¡œë”©
    if do_scroll:
        try:
            scroll_results_to_load_more(driver, max_scrolls=scroll_batches, min_increment=3, timeout_each=8)
        except Exception as e:
            print("[WARN] ìŠ¤í¬ë¡¤ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e)

    # 3) ë§í¬ ìˆ˜ì§‘
    links = collect_result_links(driver, max_items=max_items)
    print(f"[INFO] ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")

    # 4) ê° ì¥ì†Œ ì²˜ë¦¬
    for idx, (href, label) in enumerate(links, start=1):
        print(f"\n[{idx}/{len(links)}] ì´ë™: {label} â†’ {href}")
        try:
            driver.get(href)
            try:
                click_reviews(driver, timeout=25)
                print("[OK] ë¦¬ë·° ë²„íŠ¼ í´ë¦­")
            except TimeoutException:
                print("[WARN] ë¦¬ë·° ë²„íŠ¼ì„ ëª» ì°¾ìŒ (í˜ì´ì§€ HTMLë§Œ ì €ì¥)")
            save_current_html(driver, label=f"{idx:02d}_{label}_review")
        except Exception as e:
            print(f"[ERROR] ì²˜ë¦¬ ì‹¤íŒ¨({label}): {e}")

    print("\n[DONE] ëª¨ë“  í•­ëª© ì²˜ë¦¬ ì™„ë£Œ")
    input("ë¸Œë¼ìš°ì €ëŠ” ì—´ë¦° ìƒíƒœë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ Enter...")

if __name__ == "__main__":
    # ì˜ˆ) í…ŒìŠ¤íŠ¸ë¡œ 30ê°œë§Œ ìˆ˜ì§‘í•˜ë ¤ë©´ max_items=30
    run(max_items=None, do_scroll=True, scroll_batches=50)
