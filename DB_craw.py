# DB_craw.py

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama.llms import OllamaLLM
import os, re, hashlib, json
from typing import Optional, Tuple, Dict, Any, List
import math
import textwrap
import dotenv
dotenv.load_dotenv()
# ---- ì™¸ë¶€ ëª¨ë“ˆ ----
import f_multi_main_tool
import latlontest
from concurrent.futures import ThreadPoolExecutor, as_completed
# ---- sqlite3 í´ë°±(ì¼ë¶€ ìœˆë„ìš° í™˜ê²½ìš©) ----
try:
    import sqlite3  # í‘œì¤€
except Exception:
    import sys, pysqlite3  # pip install pysqlite3-binary
    sys.modules['sqlite3'] = pysqlite3
    sys.modules['_sqlite3'] = pysqlite3
    import sqlite3

# =========================
# ì„¤ì •
# =========================
DB_PATH = "reviews.db"
TOP_N_STORES = 5
STALE_DAYS = 30
PER_SOURCE_LIMIT = None   # ì¶œì²˜ë³„ ìµœì‹  Nê°œ ì œí•œ(ì—†ìœ¼ë©´ ì „ì²´)
CRAWL_MAX_REVIEWS = 10
CRAWL_HEADLESS = True
CRAWL_MAX_WORKERS = 5

PROMPT = """ë„ˆëŠ” ë¦¬ë·° ìš”ì•½ ë° í‰ê°€ ì „ë¬¸ê°€ì•¼.
ì•„ë˜ ë§¤ì¥ ë¦¬ë·°ë“¤(ì—¬ëŸ¬ ì¶œì²˜, ìµœì‹ /ê³¼ê±° í˜¼ì¬)ì„ ì½ê³ , ë°˜ë“œì‹œ ì•„ë˜ JSONë§Œ ì¶œë ¥í•´.

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ì´ í‚¤/í˜•ì‹ ê·¸ëŒ€ë¡œ):
{{
  "one_liner": "30~40ì í•µì‹¬ í•œ ì¤„ í‰",
  "rating": 4.3,
  "complain": ["ë¶ˆë§Œì‚¬í•­1", "ë¶ˆë§Œì‚¬í•­2"]
}}

ì‘ì„± ê·œì¹™:
- ì¶œë ¥ì€ **JSON í•œ ë©ì–´ë¦¬ë§Œ** ë‚´ê³ , ê·¸ ì™¸ í…ìŠ¤íŠ¸/ì„¤ëª…/ì½”ë“œë¸”ë¡(``` ë“±)ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ.
- ì–¸ì–´ëŠ” í•œêµ­ì–´. ê°„ê²°í•˜ê³  ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ. ê³¼ì¥ ê¸ˆì§€, ì´ëª¨ì§€/í•´ì‹œíƒœê·¸ ê¸ˆì§€.
- "rating"ì€ **1.0~5.0** ì‚¬ì´ **ì†Œìˆ˜ì  í•œ ìë¦¬**ì˜ ìˆ«ì(float)ë¡œ. ê·¼ê±° ë¶€ì¡±/ë¦¬ë·° ì ìœ¼ë©´ **3.0**ì— ê°€ê¹ê²Œ ë³´ìˆ˜ì .
- "complain"ì€ **ë¦¬ë·°ì— ì‹¤ì œë¡œ ë‚˜íƒ€ë‚œ ë°˜ë³µ/ë¹ˆë²ˆí•œ ë¶ˆë§Œ**ë§Œ ì¶”ì¶œ(1~2ê°œ). ì‚¬ì†Œí•œ/ë‹¨ë°œì„±ì€ ì œì™¸.
- ìƒë°˜ëœ í‰ì´ ìˆìœ¼ë©´ **ë¹ˆë„/ìµœê·¼ì„±**ì„ ê°€ë³ê²Œ ë°˜ì˜í•´ í‰ê· ì  ì²´ê° í’ˆì§ˆë¡œ íŒë‹¨.
- ì¤‘ë³µ/ë™ì˜ì–´ëŠ” í•©ì¹˜ê³ , ê° í•­ëª©ì€ **25ì ë‚´ì™¸**ë¡œ ì§§ê²Œ.
- ë§¤ì¥ëª…/ì¶œì²˜/ë³„ì  ìˆ«ì ë“± ë©”íƒ€ëŠ” ë³¸ë¬¸ì— ë„£ì§€ ë§ ê²ƒ(ì˜¤ì§ JSON í‚¤ë§Œ).

[ë§¤ì¥ëª…]
{store}

[ë¦¬ë·°ë“¤]
{reviews}
"""
# =========================
# DB & ìŠ¤í‚¤ë§ˆ (+ë§ˆì´ê·¸ë ˆì´ì…˜)
# =========================
DDL = """
PRAGMA foreign_keys = ON;
CREATE TABLE IF NOT EXISTS stores (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  store_name  TEXT,
  address     TEXT,
  lat         REAL,
  lng         REAL,
  img1        TEXT,
  img2        TEXT,
  img3        TEXT,
  store_key   TEXT UNIQUE,
  created_at  TEXT DEFAULT (datetime('now')),
  updated_at  TEXT DEFAULT (datetime('now'))
);
CREATE TABLE IF NOT EXISTS reviews (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  store_id    INTEGER NOT NULL,
  source      TEXT,
  review      TEXT,
  review_hash TEXT UNIQUE,
  first_seen  TEXT DEFAULT (datetime('now')),
  last_seen   TEXT DEFAULT (datetime('now')),
  FOREIGN KEY (store_id) REFERENCES stores(id) ON DELETE CASCADE
);
CREATE INDEX IF NOT EXISTS idx_stores_name ON stores(store_name);
CREATE INDEX IF NOT EXISTS idx_reviews_store ON reviews(store_id);
CREATE INDEX IF NOT EXISTS idx_reviews_source ON reviews(source);
CREATE INDEX IF NOT EXISTS idx_reviews_lastseen ON reviews(last_seen);
"""
def checkpoint(db_path=DB_PATH, mode="TRUNCATE"):
    with sqlite3.connect(db_path) as con:
        con.execute(f"PRAGMA wal_checkpoint({mode});")

def _connect(db_path: str = DB_PATH):
    con = sqlite3.connect(db_path)
    con.row_factory = sqlite3.Row
    con.execute("PRAGMA journal_mode=WAL;")
    con.execute("PRAGMA synchronous=NORMAL;")
    return con

def _ensure_column(table: str, col: str, col_def: str):
    with _connect() as con:
        cols = [r["name"] for r in con.execute(f"PRAGMA table_info({table})").fetchall()]
        if col not in cols:
            con.execute(f"ALTER TABLE {table} ADD COLUMN {col} {col_def};")

def _init_db():
    with _connect() as con:
        con.executescript(DDL)

# =========================
# ìœ í‹¸: ì •ê·œí™”/íŒŒì‹±/í•´ì‹œ
# =========================
def _as_float_or_none(x):
    try:
        return float(x)
    except Exception:
        return None

def _split_address_latlng(addr_field, lat=None, lng=None):
    """
    addr_field: "ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬ ..."  ë˜ëŠ”  ("ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬ ...", (37.39, 127.12))
    ë°˜í™˜: (address_str, lat, lng)
    """
    # ì¼€ì´ìŠ¤ 1) ë¬¸ìì—´ ì£¼ì†Œ
    if isinstance(addr_field, str):
        return addr_field, lat, lng

    # ì¼€ì´ìŠ¤ 2) (ì£¼ì†Œ, (lat,lng)) or [ì£¼ì†Œ, [lat,lng]]
    if isinstance(addr_field, (tuple, list)) and len(addr_field) >= 1:
        address_str = addr_field[0] if isinstance(addr_field[0], str) else str(addr_field[0])

        if len(addr_field) >= 2:
            latlng = addr_field[1]
            if isinstance(latlng, (tuple, list)) and len(latlng) >= 2:
                if lat is None:
                    lat = _as_float_or_none(latlng[0])
                if lng is None:
                    lng = _as_float_or_none(latlng[1])

        return address_str, lat, lng

    # ê¸°íƒ€: ë¬¸ìì—´ë¡œ ê°•ì œ
    return (str(addr_field) if addr_field is not None else None), lat, lng
_LATLNG_RE = re.compile(r"\(([-+]?\d+(?:\.\d+)?)[,\s]+([-+]?\d+(?:\.\d+)?)\)")

def _norm_text(s: Optional[str]) -> str:
    s = "" if s is None else str(s)
    return re.sub(r"\s+", " ", s).strip().lower()

def _parse_lat_lng_from_address(addr: Optional[str]) -> Tuple[Optional[float], Optional[float]]:
    if not isinstance(addr, str): return None, None
    m = _LATLNG_RE.search(addr)
    if not m: return None, None
    try: return float(m.group(1)), float(m.group(2))
    except Exception: return None, None

def _make_store_key(name: str, address: Optional[str]) -> str:
    base = f"{_norm_text(name)}|{_norm_text(address)}"
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

def _make_review_hash(store_id: int, source: Optional[str], review: Optional[str]) -> str:
    base = f"{store_id}|{_norm_text(source)}|{_norm_text(review)}"
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

# =========================
# Top5 í›„ë³´ (ì¹´ì¹´ì˜¤ ë¦¬ìŠ¤íŠ¸)
# =========================
def get_top5_store_pairs(keyword: str, lat: float, lon: float, query: str) -> List[Tuple[str, Optional[str], Optional[Tuple[Optional[float], Optional[float]]]]]:
    """
    ë°˜í™˜: [(store_name, address, (lat,lng)), ...] ìµœëŒ€ 5ê°œ
    f_multi_kakao_tool.run_multi() â†’ {"ë§¤ì¥ëª…": (ì£¼ì†Œ, (lat,lng))}
    """
    # âœ… ë¶„ê¸°: 'ê·¼ì²˜ ' ì ‘ë‘ì–´ â†’ GPS ë°˜ê²½ ê²€ìƒ‰ / ì•„ë‹ˆë©´ í‚¤ì›Œë“œë§Œ ê²€ìƒ‰
    kw = (keyword or "").strip()
    if kw.startswith("ê·¼ì²˜ "):
        q = kw.replace("ê·¼ì²˜", "", 1).strip()  # 'ê·¼ì²˜ ' ì œê±° â†’ ì‹¤ì œ ì¹´í…Œê³ ë¦¬/íƒœê·¸
        ret, distance = latlontest.kakao_keyword_nearby(
            lat=lat, lon=lon, query=q, TOP_N_STORES=TOP_N_STORES
        )
    else:
        # ì˜ˆ: "ì •ìë™ ì‚¼ê²¹ì‚´" â†’ ì¢Œí‘œ ì—†ì´ ì „êµ­ ê²€ìƒ‰(ì •í™•ë„ ìš°ì„ , ì¹´ì¹´ì˜¤ê°€ ì§€ì—­ì–´ë¥¼ í•´ì„)
        ret, distance = latlontest.kakao_keyword_nearby(
            query=kw, TOP_N_STORES=TOP_N_STORES
        )

    pairs: List[Tuple[str, Optional[str], Optional[Tuple[Optional[float], Optional[float]]]]] = []
    if isinstance(ret, dict):
        for name, val in list(ret.items())[:TOP_N_STORES]:
            addr, latlng = None, None
            if isinstance(val, tuple):
                if len(val) >= 1: addr = val[0]
                if len(val) >= 2 and isinstance(val[1], tuple): latlng = val[1]
            elif isinstance(val, str):
                addr = val
            pairs.append((str(name).strip(), addr, latlng))
    return pairs[:TOP_N_STORES], distance

# =========================
# ì‹ ì„ ë„/ì¡°íšŒ
# =========================
def latest_age_days(store_name: str) -> Optional[float]:
    q = """
    SELECT julianday('now') - julianday(MAX(r.last_seen)) AS age_days
    FROM reviews r
    JOIN stores s ON r.store_id = s.id
    WHERE s.store_name = ?
    """
    with _connect() as con:
        row = con.execute(q, (store_name,)).fetchone()
    if not row or row["age_days"] is None: return None
    try: return float(row["age_days"])
    except Exception: return None

# =========================
# ì—…ì„œíŠ¸(ë©”ëª¨ë¦¬ â†’ DB)  â˜… store_imageë„ ë°˜ì˜
# =========================
UPSERT_STORE_SQL = """
INSERT INTO stores (store_name, address, lat, lng, img1, img2, img3, store_key)
VALUES (?, ?, ?, ?, ?, ?, ?, ?)
ON CONFLICT(store_key) DO UPDATE SET
  store_name  = excluded.store_name,
  address     = excluded.address,
  lat         = COALESCE(excluded.lat, stores.lat),
  lng         = COALESCE(excluded.lng, stores.lng),
  img1        = COALESCE(excluded.img1, stores.img1),
  img2        = COALESCE(excluded.img2, stores.img2),
  img3        = COALESCE(excluded.img3, stores.img3),
  updated_at  = datetime('now');
"""
UPSERT_REVIEW_SQL = """
INSERT INTO reviews (store_id, source, review, review_hash)
VALUES (?, ?, ?, ?)
ON CONFLICT(review_hash) DO UPDATE SET
  source    = excluded.source,
  review    = excluded.review,
  last_seen = datetime('now');
"""

def _upsert_one_store(con, name, address, lat, lng, store_images):
    # store_images ëŠ” list ë˜ëŠ” None
    img1 = img2 = img3 = None
    if isinstance(store_images, list):
        if len(store_images) > 0: img1 = store_images[0]
        if len(store_images) > 1: img2 = store_images[1]
        if len(store_images) > 2: img3 = store_images[2]

    store_key = _make_store_key(name, address)
    con.execute(UPSERT_STORE_SQL, (name, address, lat, lng, img1, img2, img3, store_key))
    sid = con.execute("SELECT id FROM stores WHERE store_key=?", (store_key,)).fetchone()[0]
    return int(sid)

def upsert_from_results(results: dict) -> dict:
    store_ids = {}
    with _connect() as con:
        with con:  # íŠ¸ëœì­ì…˜
            for name, obj in results.items():
                address_raw = (obj or {}).get("address")      # <-- ì§€ê¸ˆì€ íŠœí”Œì¼ ìˆ˜ ìˆìŒ
                store_image = (obj or {}).get("store_image")

                # ì£¼ì†Œ ë¬¸ìì—´ ì•ˆì˜ "(lat,lng)" íŒ¨í„´ íŒŒì‹±ì´ ë”°ë¡œ ìˆë‹¤ë©´ ë¨¼ì € ì ìš© (ì˜µì…˜)
                lat0, lng0 = _parse_lat_lng_from_address(address_raw if isinstance(address_raw, str) else None)

                # â­ ìµœì¢…ì ìœ¼ë¡œ ì£¼ì†Œ/ì¢Œí‘œë¥¼ ì •ê·œí™”(ë¬¸ìì—´, float)
                address, lat, lng = _split_address_latlng(address_raw, lat0, lng0)

                # ì´ì œë¶€í„° addressëŠ” str, lat/lngëŠ” float/None ë³´ì¥
                sid = _upsert_one_store(con, name, address, lat, lng, store_image)
                store_ids[name] = sid

                for source in ("kakao", "google", "naver"):
                    reviews = ((obj or {}).get(source) or {}).get("reviews") or []
                    for rv in reviews:
                        rv_text = str(rv or "").strip()
                        if not rv_text:
                            continue
                        rh = _make_review_hash(sid, source, rv_text)
                        con.execute(UPSERT_REVIEW_SQL, (sid, source, rv_text, rh))
    return store_ids

# =========================
# í¬ë¡¤ í˜¸ì¶œ(ë§¤ì¥ ë‹¨ì¼)
# =========================
def crawl_one_store(store_name: str) -> Dict[str, Any]:
    return f_multi_main_tool.collect_all_reviews_parallel(
        keyword=store_name, top_n=1, max_reviews=CRAWL_MAX_REVIEWS, headless=CRAWL_HEADLESS
    )

# =========================
# ì¡°íšŒ: ìƒìœ„ 5ê°œë§Œ DBâ†’ë¦¬ìŠ¤íŠ¸ (store_image í¬í•¨)
# =========================
def fetch_reviews_for_store_list(store_names: List[str],
                                 per_source_limit: Optional[int] = PER_SOURCE_LIMIT) -> List[Dict[str, Any]]:
    if not store_names: return []
    placeholders = ",".join(["?"] * len(store_names))
    sql = f"""
    SELECT s.store_name, s.address, s.lat, s.lng,
       s.img1, s.img2, s.img3,
       r.source, r.review, r.last_seen
    FROM reviews r
    JOIN stores s ON r.store_id = s.id
    WHERE s.store_name IN ({placeholders})
    ORDER BY s.store_name, r.source, r.last_seen DESC
    """
    with _connect() as con:
        rows = con.execute(sql, store_names).fetchall()

    if per_source_limit and per_source_limit > 0:
        grouped = {}
        for row in rows:
            key = (row["store_name"], row["source"] or "UNKNOWN")
            grouped.setdefault(key, []).append(row)
        rows = [r for (_k, lst) in grouped.items() for r in lst[:per_source_limit]]

    out = []
    for r in rows:
        out.append({
            "store_name": r["store_name"],
            "address": r["address"],
            "lat": r["lat"],
            "lng": r["lng"],
            "img1": r["img1"],
            "img2": r["img2"],
            "img3": r["img3"],
            "source": r["source"],
            "review": r["review"],
            "last_seen": r["last_seen"],
        })
    return out

def _second_token_or_first(q: str) -> Optional[str]:
    toks = (q or "").split()
    if not toks:
        return None
    return toks[1] if len(toks) >= 2 else toks[0]

# =========================
# ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜(í‚¤ì›Œë“œ ì „ìš©, LLM ì—†ìŒ)
# =========================
def run_keyword_flow(keyword: str, lat: float, lon:float, query:str,
                     stale_days: int = STALE_DAYS,
                     per_source_limit: Optional[int] = PER_SOURCE_LIMIT) -> Dict[str, Any]:
    def _extract_dong(text: str) -> Optional[str]:
        """ë¬¸ìì—´ì—ì„œ 'ì •ìë™', 'ì•¼íƒ‘ë™' ê°™ì€ ë™ í† í°ì„ ì¶”ì¶œ"""
        if not text:
            return None
        m = re.search(r'([ê°€-í£0-9]+ë™)\b', text)
        return m.group(1) if m else None
    _init_db()

    # 1) Top-N í›„ë³´
    top5_pairs, distance = get_top5_store_pairs(keyword, lat, lon, query)
    top5_names = [n for (n, _a, _ll) in top5_pairs]
    print(f"[INFO] Top-{len(top5_names)} stores:", ", ".join(top5_names))

    # 2) Freshness ì²´í¬ â†’ í•„ìš”ì‹œ í¬ë¡¤ë§ & ì—…ì„œíŠ¸
    need_crawl: List[str] = []
    for name, addr, latlng in top5_pairs:
        age = latest_age_days(name)
        if age is None or age > stale_days:
            need_crawl.append(name)

    if need_crawl:
        all_results: Dict[str, Any] = {}

        # âœ… í˜ì–´ ë§µ(ì£¼ì†Œ/ì¢Œí‘œë¥¼ ë‚˜ì¤‘ì— upsert ì§ì „ì—ë„ ì“¸ ê±°ë¼ ë£¨í”„ ë°–ì—ì„œ ë§Œë“¤ì–´ë‘ )
        pair_map = {n: (a, ll) for (n, a, ll) in top5_pairs}

        to_crawl: List[Tuple[str, str]] = []
        for name in need_crawl:
            try:
                addr, _latlng = pair_map.get(name, (None, None))

                # 1) ì£¼ì†Œì—ì„œ ë™ ì¶”ì¶œ â†’ ì—†ìœ¼ë©´ keywordì—ì„œ ì¶”ì¶œ
                dong = _extract_dong(addr) or _extract_dong(keyword)

                # 2) ë§¤ì¥ëª…ì˜ ì²« í† í° + ë™ì„ ì¡°í•© (ë™ì´ ìˆìœ¼ë©´ ì•ì— ë¶™ì„)
                base_token = (name.split()[0] if name else "").strip()
                if dong:
                    n_keyword = f"{dong} {base_token}".strip()
                else:
                    n_keyword = base_token or name  # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ name ì „ì²´

                to_crawl.append((name, n_keyword))
            except Exception as e:
                print(f"[PREP_ERROR] {name}: {e}")

        # 2) ë³‘ë ¬ í¬ë¡¤ (I/O ë°”ìš´ë“œ)
        with ThreadPoolExecutor(max_workers=CRAWL_MAX_WORKERS, thread_name_prefix="crawl") as ex:
            future_map = {ex.submit(crawl_one_store, nkw): name for (name, nkw) in to_crawl}

            for fut in as_completed(future_map):
                name = future_map[fut]
                try:
                    res = fut.result()  # ê¸°ëŒ€í˜•íƒœ: { "ë§¤ì¥ëª…": {...} }
                    if isinstance(res, dict) and res:
                        all_results.update(res)
                except Exception as e:
                    print(f"[CRAWL_ERROR] {name}: {e}")

        if all_results:
            # âœ… ì—…ì„œíŠ¸ ì§ì „ ì¢Œí‘œ ì£¼ì… (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            for n, obj in all_results.items():
                if n in pair_map:
                    a, ll = pair_map[n]
                    obj["address"] = (a, ll)

            upsert_from_results(all_results)
            checkpoint(db_path=DB_PATH)

    # 3) DBì—ì„œ ìµœì¢… ë¦¬ë·° ë¶ˆëŸ¬ì˜¤ê¸°
    rows = fetch_reviews_for_store_list(top5_names, per_source_limit=per_source_limit)

    # 4) ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    results: Dict[str, Any] = {}
    for r in rows:
        store = r["store_name"]
        if store not in results:
            images = [x for x in (r["img1"], r["img2"], r["img3"]) if x]
            results[store] = {
                "address": r["address"],
                "lat": r["lat"],
                "lng": r["lng"],
                "store_image": images,
                "kakao": {"reviews": []},
                "google": {"reviews": []},
                "naver": {"reviews": []}
            }
        src = (r["source"] or "").lower()
        if src in results[store]:
            results[store][src]["reviews"].append(r["review"])

    return results, distance

def _safe_parse_json(raw: str) -> Dict[str, Any]:
    try:
        return json.loads(raw)
    except Exception:
        m = re.search(r"\{[\s\S]*\}", raw)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                pass
    return {}

def _sanitize_payload(parsed: Dict[str, Any], raw_text: str) -> Dict[str, Any]:
    one  = parsed.get("one_liner") or ""
    one  = one.strip() if isinstance(one, str) else ""

    rating = parsed.get("rating", 3.0)
    try:
        rating = float(rating)
    except Exception:
        rating = 3.0
    rating = max(1.0, min(5.0, rating))

    complains = [c for c in parsed.get("complain", []) if isinstance(c, str) and c.strip()][:6]

    return {
        "one_liner": one,
        "rating": round(rating, 1),
        "complain": complains,
        "raw_text_len": len(raw_text),
    }

def _interleave_and_dedupe(buckets: List[List[str]], limit: int) -> List[str]:
    """ì—¬ëŸ¬ ì†ŒìŠ¤ ë¦¬ë·°ë¥¼ ë²ˆê°ˆì•„ ì„ì–´ limitê¹Œì§€ ìˆ˜ì§‘, ê³µë°±/ì¤‘ë³µ ì œê±°."""
    idx = [0] * len(buckets)
    out, seen = [], set()
    while len(out) < limit and any(i < len(b) for i, b in zip(idx, buckets)):
        for k, b in enumerate(buckets):
            i = idx[k]
            if i < len(b):
                rv = (b[i] or "").strip()
                idx[k] += 1
                if not rv or rv in seen:
                    continue
                seen.add(rv)
                out.append(rv)
                if len(out) >= limit:
                    break
    return out

def _gather_reviews_per_store(data: Dict[str, Any], limit: int) -> List[str]:
    """results[store]ì—ì„œ ë¦¬ë·°ë§Œ ì¶”ì¶œ. kakao/google/naver + ìµœìƒë‹¨ reviews ëª¨ë‘ ì§€ì›."""
    # í‘œì¤€ êµ¬ì¡°
    kakao  = list((((data.get("kakao")  or {}).get("reviews")) or []))
    google = list((((data.get("google") or {}).get("reviews")) or []))
    naver  = list((((data.get("naver")  or {}).get("reviews")) or []))
    # fallback: ìµœìƒë‹¨ reviews
    top    = list(((data.get("reviews")) or []))
    buckets = [kakao, google, naver, top] if top else [kakao, google, naver]
    return _interleave_and_dedupe(buckets, limit)

# -------------------- ë©”ì¸ í•¨ìˆ˜ --------------------
def summarize_store_with_rating(
    results: Dict[str, Any],
    model_name: str = "llama3.1",
    max_reviews_per_store: int = 60,
    max_workers: int = 6,
    temperature: float = 0.0,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None
) -> Dict[str, Any]:
    """
    dict(results)ë¥¼ ë°›ì•„ LLMì— ë³‘ë ¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•´ ë§¤ì¥ë³„ í•œ ì¤„ í‰/ì¥ë‹¨ì /ë³„ì ì„ ìƒì„±.
    return: {store: {"one_liner": str, "pros":[...], "cons":[...], "rating":4.2, "raw_text_len":N}}
    """
    llm = OllamaLLM(
        model=model_name,
        temperature=temperature,
        # api_key=api_key or os.getenv('OLLAMA_LOCAL_HOST'),
        base_url=base_url
    )
    prompt = PromptTemplate.from_template(PROMPT)
    chain = prompt | llm | StrOutputParser()

    # ì…ë ¥ ì¤€ë¹„
    inputs, order = [], []
    for store, data in results.items():
        reviews = _gather_reviews_per_store(data, max_reviews_per_store)
        text = "\n".join(reviews)
        inputs.append({"store": store, "reviews": text})
        order.append((store, text))

    if not inputs:
        return {}

    # ë³‘ë ¬ ì¶”ë¡  (LCEL .batch)
    raw_outputs = chain.batch(inputs, config={"max_concurrency": max_workers})
    # ì„ì‹œ
    with open("result.txt", 'w', encoding='utf-8') as f:
        f.write(str(raw_outputs))
    # í›„ì²˜ë¦¬
    out: Dict[str, Any] = {}
    for (store, text), raw in zip(order, raw_outputs):
        parsed = _safe_parse_json(raw)
        # ë¦¬ë·°ê°€ ë¹„ì–´ìˆê±°ë‚˜ JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        if not text.strip():
            out[store] = {"one_liner": "", "rating": 3.0, "complain": [], "raw_text_len": 0}
            continue
        if not parsed or not isinstance(parsed, dict):
            parsed = {"one_liner": "", "rating": 3.0, "complain": [], "raw_text_len": 0}
        out[store] = _sanitize_payload(parsed, text)

    return out
def pretty_print_summaries(summary: dict, width: int = 80):
    """
    summary: { "ê°€ê²Œëª…": {"one_liner": str, "rating": float, "complain": [str], "raw_text_len": int}, ... }
    width  : í•œ ì¤„ ë˜í•‘ í­
    """
    if not summary:
        print("ì¶œë ¥í•  ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    name_w = max(len(str(name)) for name in summary.keys())

    for store, s in summary.items():
        one = (s.get("one_liner") or "").strip()
        rating = float(s.get("rating", 3.0))
        complains = [c for c in (s.get("complain") or []) if isinstance(c, str) and c.strip()]
        raw_len = int(s.get("raw_text_len", 0))

        # ë³„ì  ë§‰ëŒ€
        filled = int(round(rating))  # ì •ìˆ˜ ê°œìˆ˜ì˜ ë³„
        stars = "â˜…" * filled + "â˜†" * (5 - filled)
        stars += f"  ({rating:.1f})"

        # í—¤ë”
        print("â”€" * width)
        print(f"ğŸª {store}")
        print(f"   ë³„ì : {stars}   |   ì›ë¬¸ ê¸¸ì´: {raw_len}")

        # í•œ ì¤„ í‰
        if one:
            wrapped = textwrap.fill(one, width=width, subsequent_indent=" " * 6)
            print(f"   í•œì¤„í‰: {wrapped}")
        else:
            print(f"   í•œì¤„í‰: (ì—†ìŒ)")

        # ë¶ˆë§Œì‚¬í•­
        if complains:
            print("   ë¶ˆë§Œì‚¬í•­:")
            for i, c in enumerate(complains, 1):
                wrapped = textwrap.fill(c, width=width, subsequent_indent=" " * 8)
                print(f"      {i}. {wrapped}")
        else:
            print("   ë¶ˆë§Œì‚¬í•­: (ì—†ìŒ)")

    print("â”€" * width)
if __name__ == "__main__":
    out = run_keyword_flow("ì •ì ë‘í–¥", stale_days=7, per_source_limit=None)
    # print(out)
    # summary = summarize_store_with_rating(
    #     results=out,  # ì• ë‹¨ê³„ ì‚°ì¶œë¬¼
    #     model_name="llama3.1",
    #     max_reviews_per_store=60,
    #     max_workers=6,
    #     temperature=0.2,  # ì¼ê´€ëœ ì¶œë ¥
    #     base_url= os.getenv("OLLAMA_REMOTE_HOST")
    # )
    # pretty_print_summaries(summary, width=90)
    from pprint import pprint
    pprint(out)